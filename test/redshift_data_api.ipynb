{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones para interaccionar con AWS Redshift desde python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def table_to_dataframe(table,schema,database='landing_zone', NUM_ENTRIES = 0, cluster_identifier = 'redshift-data', db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Amazon Redshift y devuelve los resultados como un DataFrame de pandas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - table : Tabla a copiar en un dataframe\n",
    "    - schema: schema de la tabla a copiar en un datafrmae\n",
    "    - database: database de la tabla a copiar en un dataframe\n",
    "    - LIMIT: limitar entries\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    - database: Nombre de la base de datos.\n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    sql_query = f\"SELECT * FROM {database}.{schema}.{table}\"\n",
    "    if NUM_ENTRIES != 0:\n",
    "        sql_query = sql_query + f\"LIMIT {NUM_ENTRIES}\"\n",
    "        \n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    \n",
    "    if status == 'FINISHED':\n",
    "        response1 = client.get_statement_result(Id=statement_id)\n",
    "        \n",
    "        # Extrayendo los nombres de las columnas de la metadata de columnas\n",
    "        column_metadata = response1['ColumnMetadata']\n",
    "        column_names = [column['name'] for column in column_metadata]\n",
    "        \n",
    "        # Construyendo el DataFrame\n",
    "        df = pd.DataFrame([[field.get('stringValue', '') for field in record] for record in response1['Records']], columns=column_names)\n",
    "        \n",
    "        return df\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "    return pd.DataFrame()  # Retorna un DataFrame vacío si la consulta falla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FINISHED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>months_frequency</th>\n",
       "      <th>discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id months_frequency discount\n",
       "0                   1        0\n",
       "1                  12      0.4\n",
       "2                   6     0.25"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = table_to_dataframe(\"frequency\",\"public\",\"landing_zone\")\n",
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FAILED\n",
      "Error: ERROR: Could not find parent table for alias \"landing_zone.pu123blic.frequency\".\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = table_to_dataframe(\"frequency\",\"pu123blic\",\"landing_zone\")\n",
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volcar el contenido de una query en un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def query_to_dataframe(sql_query, cluster_identifier = 'redshift-data', database = \"landing_zone\", db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Amazon Redshift y devuelve los resultados como un DataFrame de pandas.\n",
    "    \n",
    "    Parámetros:\n",
    "    - sql_query: Consulta SQL para ejecutar.\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    - database: Nombre de la base de datos.\n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    \n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    \n",
    "    if status == 'FINISHED':\n",
    "        response1 = client.get_statement_result(Id=statement_id)\n",
    "        \n",
    "        # Extrayendo los nombres de las columnas de la metadata de columnas\n",
    "        column_metadata = response1['ColumnMetadata']\n",
    "        column_names = [column['name'] for column in column_metadata]\n",
    "        \n",
    "        # Construyendo el DataFrame\n",
    "        df = pd.DataFrame([[field.get('stringValue', '') for field in record] for record in response1['Records']], columns=column_names)\n",
    "        \n",
    "        return df\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "    return pd.DataFrame()  # Retorna un DataFrame vacío si la consulta falla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FINISHED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>months_frequency</th>\n",
       "      <th>discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id months_frequency discount\n",
       "0                   1        0\n",
       "1                  12      0.4\n",
       "2                   6     0.25"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_1 = query_to_dataframe(\"SELECT * FROM landing_zone.public.frequency;\", cluster_identifier = 'redshift-data', database = \"landing_zone\", db_user = 'admintreinta')\n",
    "DATA_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FAILED\n",
      "Error: ERROR: syntax error at or near \"FROFDM\"\n",
      "  Position: 10\n"
     ]
    }
   ],
   "source": [
    "DATA_1 = query_to_dataframe(\"SELECT * FROFDM landing_zone.public.frequency;\", cluster_identifier = 'redshift-data', database = \"landing_zone\", db_user = 'admintreinta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subimos un dataframe a s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import datetime\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "def dataframe_to_s3(df, bucket, endpoint='data_lake', object_name=''):\n",
    "    # Generar un sello de tiempo con el formato deseado\n",
    "    timestamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S_%f')\n",
    "    year = datetime.datetime.now().strftime('%Y')\n",
    "    month = datetime.datetime.now().strftime('%m')\n",
    "    day = datetime.datetime.now().strftime('%d')\n",
    "    # Generar un identificador único (puedes reemplazarlo por cualquier otra cadena aleatoria si prefieres)\n",
    "    if object_name == '':\n",
    "        object_name = uuid.uuid4().hex\n",
    "        \n",
    "    object_path = f\"{endpoint}/{year}/{month}/{day}/{object_name}_{timestamp}.csv.gz\"\n",
    "\n",
    "    # Usar BytesIO para datos binarios\n",
    "    csv_buffer = BytesIO()\n",
    "    df.to_csv(csv_buffer, index=False, compression='gzip')\n",
    "    \n",
    "    # Es necesario mover el puntero del buffer al inicio después de escribir en él\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, object_path).put(Body=csv_buffer.getvalue())\n",
    "    \n",
    "    return f's3://{bucket}/{object_path}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   columna1\n",
      "0        42\n",
      "1        27\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'columna1': [42, 27]  # Dos valores tipo int\n",
    "})\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ms-redshift-core/data_lake/2024/01/24/576f049edaf248749ad35c61d8143ad9_2024_01_24_15_30_47_080742.csv.gz'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_to_s3(df_test,\"ms-redshift-core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchBucket",
     "evalue": "An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataframe_to_s3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mms-redsh3ift-core\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 27\u001b[0m, in \u001b[0;36mdataframe_to_s3\u001b[0;34m(df, bucket, endpoint, object_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m csv_buffer\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m s3_resource \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mresource(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m \u001b[43ms3_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mObject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/coding/treinta/redshift_python/redshift_utils/lib/python3.11/site-packages/boto3/resources/factory.py:581\u001b[0m, in \u001b[0;36mResourceFactory._create_action.<locals>.do_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# Clear cached data. It will be reloaded the next\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# time that an attribute is accessed.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# TODO: Make this configurable in the future?\u001b[39;00m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/coding/treinta/redshift_python/redshift_utils/lib/python3.11/site-packages/boto3/resources/action.py:88\u001b[0m, in \u001b[0;36mServiceAction.__call__\u001b[0;34m(self, parent, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     81\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     parent\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mservice_name,\n\u001b[1;32m     84\u001b[0m     operation_name,\n\u001b[1;32m     85\u001b[0m     params,\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, response)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_handler(parent, params, response)\n",
      "File \u001b[0;32m~/coding/treinta/redshift_python/redshift_utils/lib/python3.11/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/coding/treinta/redshift_python/redshift_utils/lib/python3.11/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "dataframe_to_s3(df_test1,\"ms-redsh3ift-core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "def load_s3_to_redshift(table,schema, s3_object_path, database='landing_zone', cluster_identifier='redshift-data', db_user='admintreinta'):\n",
    "    client = boto3.client('redshift-data')\n",
    "    sql = f\"\"\"\n",
    "        COPY {database}.{schema}.{table}\n",
    "        FROM '{s3_object_path}'\n",
    "        IAM_ROLE default\n",
    "        delimiter ','\n",
    "        IGNOREHEADER 1\n",
    "        GZIP\n",
    "        CSV;\n",
    "    \"\"\"\n",
    "\n",
    "    print(sql)\n",
    "    # Ejecuta el comando COPY\n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql\n",
    "    )\n",
    "    \n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera a que la ejecución termine\n",
    "    status = 'STARTED'\n",
    "    while status in ['SUBMITTED', 'STARTED', 'PICKED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de consultar nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Estado actual: {status}\")\n",
    "\n",
    "    # Verifica el resultado de la ejecución\n",
    "    if status == 'FINISHED':\n",
    "        print(\"La carga ha sido exitosa.\")\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error al truncar la tabla: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "    \n",
    "        \n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        COPY landing_zone.public.test1245\n",
      "        FROM 's3://ms-redshift-core/data_lake/2024/01/24/f21e65852e944a2f8516075de529ecd6_2024_01_24_15_19_13_372618.csv.gz'\n",
      "        IAM_ROLE default\n",
      "        delimiter ','\n",
      "        IGNOREHEADER 1\n",
      "        GZIP\n",
      "        CSV;\n",
      "    \n",
      "Estado actual: FINISHED\n",
      "La carga ha sido exitosa.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'redshift-data',\n",
       " 'CreatedAt': datetime.datetime(2024, 1, 24, 15, 24, 31, 212000, tzinfo=tzlocal()),\n",
       " 'Database': 'landing_zone',\n",
       " 'DbUser': 'admintreinta',\n",
       " 'Id': '4364cebe-03c6-4940-9e55-6d5db9bea0e8',\n",
       " 'ResponseMetadata': {'RequestId': '4364cebe-03c6-4940-9e55-6d5db9bea0e8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4364cebe-03c6-4940-9e55-6d5db9bea0e8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '160',\n",
       "   'date': 'Wed, 24 Jan 2024 20:24:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_path_test = 's3://ms-redshift-core/data_lake/2024/01/24/f21e65852e944a2f8516075de529ecd6_2024_01_24_15_19_13_372618.csv.gz'\n",
    "load_s3_to_redshift(\"test1245\", \"public\", s3_path_test, cluster_identifier='redshift-data', database='landing_zone', db_user='admintreinta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        COPY test1245\n",
      "        FROM 's12313://ms-redshift-core/data_lake/2024/01/24/f21e65852e944a2f8516075de529ecd6_2024_01_24_15_19_13_372618.csv.gz'\n",
      "        IAM_ROLE default\n",
      "        delimiter ','\n",
      "        IGNOREHEADER 1\n",
      "        GZIP\n",
      "        CSV;\n",
      "    \n",
      "Estado actual: FAILED\n",
      "Error al truncar la tabla: ERROR: LOAD source is not supported. (Hint: only S3 or DynamoDB or EMR based load is allowed)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'redshift-data',\n",
       " 'CreatedAt': datetime.datetime(2024, 1, 24, 15, 22, 3, 60000, tzinfo=tzlocal()),\n",
       " 'Database': 'landing_zone',\n",
       " 'DbUser': 'admintreinta',\n",
       " 'Id': '507e6572-d672-4863-9d06-8a86d4dcf152',\n",
       " 'ResponseMetadata': {'RequestId': '507e6572-d672-4863-9d06-8a86d4dcf152',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '507e6572-d672-4863-9d06-8a86d4dcf152',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '159',\n",
       "   'date': 'Wed, 24 Jan 2024 20:22:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_path_test = 's12313://ms-redshift-core/data_lake/2024/01/24/f21e65852e944a2f8516075de529ecd6_2024_01_24_15_19_13_372618.csv.gz'\n",
    "load_s3_to_redshift(\"test1245\", \"public\",s3_path_test, cluster_identifier='redshift-data', database='landing_zone', db_user='admintreinta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_redshift(df,table,schema,bucket,database='landing_zone',endpoint = 'data_lake',object_name = False,db_user ='admintreinta', cluster_identifier = 'redshift-data'):\n",
    "    s3_object_path = dataframe_to_s3(df, bucket, endpoint, object_name)\n",
    "    output = load_s3_to_redshift(table,schema, s3_object_path, cluster_identifier, database, db_user)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'columna1': [42, 27]  # Dos valores tipo int\n",
    "})\n",
    "\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        COPY landing_zone.public.test1245\n",
      "        FROM 's3://ms-redshift-core/data_lake/2024/01/24/False_2024_01_24_15_26_16_764772.csv.gz'\n",
      "        IAM_ROLE default\n",
      "        delimiter ','\n",
      "        IGNOREHEADER 1\n",
      "        GZIP\n",
      "        CSV;\n",
      "    \n",
      "Estado actual: FINISHED\n",
      "La carga ha sido exitosa.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'redshift-data',\n",
       " 'CreatedAt': datetime.datetime(2024, 1, 24, 15, 26, 19, 222000, tzinfo=tzlocal()),\n",
       " 'Database': 'landing_zone',\n",
       " 'DbUser': 'admintreinta',\n",
       " 'Id': '48972e79-5fa9-4800-9f0d-39953bedfd5f',\n",
       " 'ResponseMetadata': {'RequestId': '48972e79-5fa9-4800-9f0d-39953bedfd5f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '48972e79-5fa9-4800-9f0d-39953bedfd5f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '160',\n",
       "   'date': 'Wed, 24 Jan 2024 20:26:19 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_to_redshift(df_test,\"test1245\",\"public\",\"ms-redshift-core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        COPY landing_zone.public.test113245\n",
      "        FROM 's3://ms-redshift-core/data_lake/2024/01/24/False_2024_01_24_15_26_36_942945.csv.gz'\n",
      "        IAM_ROLE default\n",
      "        delimiter ','\n",
      "        IGNOREHEADER 1\n",
      "        GZIP\n",
      "        CSV;\n",
      "    \n",
      "Estado actual: FAILED\n",
      "Error al truncar la tabla: ERROR: Cannot COPY into nonexistent table test113245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ClusterIdentifier': 'redshift-data',\n",
       " 'CreatedAt': datetime.datetime(2024, 1, 24, 15, 26, 39, 765000, tzinfo=tzlocal()),\n",
       " 'Database': 'landing_zone',\n",
       " 'DbUser': 'admintreinta',\n",
       " 'Id': '4fa9f183-34bc-47f2-865d-de3b6883bc3f',\n",
       " 'ResponseMetadata': {'RequestId': '4fa9f183-34bc-47f2-865d-de3b6883bc3f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4fa9f183-34bc-47f2-865d-de3b6883bc3f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '160',\n",
       "   'date': 'Wed, 24 Jan 2024 20:26:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_to_redshift(df_test,\"test113245\",\"public\",\"ms-redshift-core\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_SP(store_procedure,schema,database = \"landing_zone\", cluster_identifier = 'redshift-data', db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una un store en Amazon Redshift\n",
    "    \n",
    "    Parámetros:\n",
    "    - store_procedure: nombre del store procedure a ejecutar\n",
    "    - schema: Esquema en el que se encuentra el store procedure.\n",
    "    - database: Nombre de la base de datos.\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    \n",
    "    sql_query = f\"CALL {database}.{schema}.{store_procedure}()\"\n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    if status == 'FINISHED':\n",
    "        print ('Store Procedure ejecutado')\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(error_message)\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_table(table, schema, database = \"landing_zone\", cluster_identifier = 'redshift-data', db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una un store en Amazon Redshift\n",
    "    \n",
    "    Parámetros:\n",
    "    - table: tabla a truncar\n",
    "    - schema: Esquema en el que se encuentra el store procedure.\n",
    "    - store_procedure: nombre del store procedure a ejecutar\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    - database: Nombre de la base de datos.\n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    \n",
    "    sql_query = f\"TRUNCATE {database}.{schema}.{table}\"\n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    \n",
    "    if status == 'FINISHED':\n",
    "        print ('Store Procedure ejecutado')\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error al truncar la tabla: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FINISHED\n",
      "Store Procedure ejecutado\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_table(\"test1245\",\"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FAILED\n",
      "Error al truncar la tabla: ERROR: relation \"public.test1245123\" does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncate_table(\"test1245123\",\"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP de una tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_table(table, schema, database = \"landing_zone\", cluster_identifier = 'redshift-data', db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una un store en Amazon Redshift\n",
    "    \n",
    "    Parámetros:\n",
    "    - table: tabla a truncar\n",
    "    - schema: Esquema en el que se encuentra el store procedure.\n",
    "    - database: Nombre de la base de datos.\n",
    "    - store_procedure: nombre del store procedure a ejecutar\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    \n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    \n",
    "    sql_query = f\"DROP TABLE {database}.{schema}.{table}\"\n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    \n",
    "    if status == 'FINISHED':\n",
    "        print ('Taabla eliminada!')\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error al eliminar la tabla: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FINISHED\n",
      "Taabla eliminada!\n"
     ]
    }
   ],
   "source": [
    "drop_table(\"test1245\",\"public\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FAILED\n",
      "Error al eliminar la tabla: ERROR: Table \"test1241245\" does not exist\n"
     ]
    }
   ],
   "source": [
    "drop_table(\"test1241245\",\"public\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_query(sql_query, database = \"landing_zone\",cluster_identifier = 'redshift-data', db_user = 'admintreinta'):\n",
    "    \"\"\"\n",
    "    Ejecuta una un store en Amazon Redshift\n",
    "    \n",
    "    Parámetros:\n",
    "    - sql_query: query a ejecutar en SQL\n",
    "    - database: Nombre de la base de datos.\n",
    "    - cluster_identifier: Identificador del cluster de Amazon Redshift.\n",
    "    - db_user: Usuario de la base de datos.\n",
    "    \n",
    "    \n",
    "    Retorna:\n",
    "    - Un DataFrame de pandas con los resultados de la consulta.\n",
    "    \"\"\"\n",
    "    client = boto3.client('redshift-data')\n",
    "    \n",
    "    response = client.execute_statement(\n",
    "        ClusterIdentifier=cluster_identifier,\n",
    "        Database=database,\n",
    "        DbUser=db_user,\n",
    "        Sql=sql_query\n",
    "    )\n",
    "\n",
    "    statement_id = response['Id']\n",
    "    \n",
    "    # Espera hasta que la consulta se haya completado\n",
    "    status = ''\n",
    "    while status not in ['FINISHED', 'FAILED', 'ABORTED']:\n",
    "        time.sleep(5)  # Espera 5 segundos antes de verificar el estado nuevamente\n",
    "        status_response = client.describe_statement(Id=statement_id)\n",
    "        status = status_response['Status']\n",
    "        print(f\"Current status: {status}\")\n",
    "    \n",
    "    if status == 'FINISHED':\n",
    "        print ('Query ejecutada!')\n",
    "    elif status == 'FAILED':\n",
    "        # Obtiene y muestra el mensaje de error\n",
    "        error_message = status_response.get('Error', 'No se proporcionó información de error.')\n",
    "        print(f\"Error ejecutando la query SQL: {error_message}\")\n",
    "    else:\n",
    "        print(\"La operación fue abortada o no se completó exitosamente.\")\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status: FINISHED\n",
      "Taabla eliminada!\n"
     ]
    }
   ],
   "source": [
    "sql_query(\"CREATE TABLE public.test1245 (columna1 integer ENCODE az64) DISTSTYLE AUTO;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
